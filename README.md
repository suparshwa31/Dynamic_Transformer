# Dynamic_Transformer

This repository provides code and resources for implementing dynamic configurations of transformer models, focusing on T5 and BART architectures. The objective is to explore dynamic model configuration techniques to enhance the adaptability and efficiency of transformer-based models.

## Overview
The project involves utilizing transformer models to dynamically adjust their configurations for various natural language processing tasks. By modifying aspects such as layer depth, attention heads, and feedforward dimensions, the models can be tailored to specific requirements, potentially improving performance and computational efficiency.

## Dependencies

- Python 3.6 or higher
- PyTorch
- Transformers library from Hugging Face
- Jupyter Notebook

## Project Description

The repository includes Jupyter Notebooks demonstrating dynamic configurations with T5 and BART models:

T5 Model Configuration: The MISHA02_Dynamic Model Configuration T5.ipynb notebook showcases how to adjust the T5 model's parameters dynamically to suit different tasks.
BART Model Configuration: The custom_BART_(2)-4.ipynb notebook illustrates dynamic configuration techniques applied to the BART model, highlighting modifications in its architecture for task-specific optimization.
